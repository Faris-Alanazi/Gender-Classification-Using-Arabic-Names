{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python=3.8\n",
    "# conda env name : gender_pred_env\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_pickle('data/dataset_after_preporcessing.pkl')\n",
    "\n",
    "max_name_length = max(df['name'].apply(len))\n",
    "unique_chars = set(''.join(df['name']))  \n",
    "vocab_size = len(unique_chars) + 1  \n",
    "\n",
    "with open('saved_models/tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['name'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_name_length)\n",
    "first_letter = df['name'].apply(lambda x: x[0])\n",
    "last_letter = df['name'].apply(lambda x: x[-1])\n",
    "\n",
    "first_letter_encoded = np.array([ord(char) for char in first_letter])\n",
    "last_letter_encoded = np.array([ord(char) for char in last_letter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the scaler models\n",
    "scaler_first = joblib.load('saved_models/scaler_models/scaler_first_letter.pkl')\n",
    "scaler_last = joblib.load('saved_models/scaler_models/scaler_last_letter.pkl')\n",
    "first_letter_encoded_scaled = scaler_first.transform(first_letter_encoded.reshape(-1, 1))\n",
    "last_letter_encoded_scaled = scaler_last.transform(last_letter_encoded.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the final dataset\n",
    "y = df['sex'].values\n",
    "X = np.concatenate([padded_sequences, first_letter_encoded_scaled, last_letter_encoded_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "total_features_shape = X.shape[1]\n",
    "embedding_dim = 256\n",
    "lstm_units = 128\n",
    "l2_lambda = 0.000\n",
    "dropout_rate = 0.25\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "patience_for_early_stopping = 5\n",
    "k = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 19, 256)           9472      \n",
      "                                                                 \n",
      " bidirectional_22 (Bidirecti  (None, 19, 256)          394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 19, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_23 (Bidirecti  (None, 128)              164352    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 568,193\n",
      "Trainable params: 568,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=total_features_shape))\n",
    "\n",
    "model.add(Bidirectional(LSTM(lstm_units,return_sequences=True)))\n",
    "\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Bidirectional(LSTM(lstm_units//2)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=patience_for_early_stopping, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture_str = []\n",
    "\n",
    "# Define a function to append each line of the model summary to the list\n",
    "def append_model_summary(line):\n",
    "    model_architecture_str.append(line)\n",
    "\n",
    "# Generate model summary\n",
    "model.summary(print_fn=append_model_summary)\n",
    "\n",
    "# Convert list to string\n",
    "model_architecture_str = '\\n'.join(model_architecture_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Test\")\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists for fold results\n",
    "fold_train_accuracies = []\n",
    "fold_val_accuracies = []\n",
    "fold_fpr = []\n",
    "fold_tpr = []\n",
    "fold_roc_auc = []\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "fold_f1_scores = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_roc_aucs = []\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"lstm_units\": lstm_units,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"l2_reg\": l2_lambda,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"patience_for_early_stopping\":patience_for_early_stopping,\n",
    "        \"k-folds\":k\n",
    "    })\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "\n",
    "        # Predictions for additional metrics\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        y_pred_classes = np.where(y_pred > 0.5, 1, 0).reshape(-1)\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        f1 = f1_score(y_val_fold, y_pred_classes)\n",
    "        precision = precision_score(y_val_fold, y_pred_classes)\n",
    "        recall = recall_score(y_val_fold, y_pred_classes)\n",
    "        roc_auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_val_fold, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Log fold-specific metrics\n",
    "        mlflow.log_metrics({\n",
    "            f\"fold_{fold+1}_accuracy\": val_accuracy,\n",
    "            f\"fold_{fold+1}_loss\": val_loss,\n",
    "            f\"fold_{fold+1}_f1_score\": f1,\n",
    "            f\"fold_{fold+1}_precision\": precision,\n",
    "            f\"fold_{fold+1}_recall\": recall,\n",
    "            f\"fold_{fold+1}_roc_auc\": roc_auc,\n",
    "        }, step=fold)\n",
    "\n",
    "        # Store the history\n",
    "        fold_train_accuracies.append(history.history['accuracy'])\n",
    "        fold_val_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "        # Store metrics for average calculation\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        fold_losses.append(val_loss)\n",
    "        fold_f1_scores.append(f1)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_roc_aucs.append(roc_auc)\n",
    "        fold_fpr.append(fpr)\n",
    "        fold_tpr.append(tpr)\n",
    "        fold_roc_auc.append(roc_auc)\n",
    "\n",
    "        # Reset model states\n",
    "        model.reset_states()\n",
    "\n",
    "    # Calculate average metrics across all folds\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    avg_f1 = np.mean(fold_f1_scores)\n",
    "    avg_precision = np.mean(fold_precisions)\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    avg_roc_auc = np.mean(fold_roc_aucs)\n",
    "    avg_train_accuracies = np.mean(fold_train_accuracies, axis=0)\n",
    "    avg_val_accuracies = np.mean(fold_val_accuracies, axis=0)\n",
    "\n",
    "    # Log average metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"avg_accuracy\": avg_accuracy,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"avg_f1_score\": avg_f1,\n",
    "        \"avg_precision\": avg_precision,\n",
    "        \"avg_recall\": avg_recall,\n",
    "        \"avg_roc_auc\": avg_roc_auc,\n",
    "    })\n",
    "\n",
    "    signature = infer_signature(X, model.predict(X))\n",
    "    # Log the model\n",
    "    mlflow.keras.log_model(model, \"model\",signature=signature)\n",
    "\n",
    "        # Set additional tags\n",
    "    mlflow.set_tags({\n",
    "        \"Description\": f\"Character-Level BiLSTM with {k}-fold cross validation\",\n",
    "        \"Encoding\": \"Character-Level for name | other features Converting them to their ASCII value then norm\",\n",
    "        \"Features\": ', '.join(df.columns.tolist()),\n",
    "        'Number of Features': len(df.columns.tolist()),\n",
    "        \"Model Type\": \"BiLSTM\",\n",
    "        \"model_architecture\": model_architecture_str\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average metrics\n",
    "print()\n",
    "print('-----------------------------------------------------------')\n",
    "print(f\"Average Validation Accuracy: {round(avg_accuracy, 3)}\")\n",
    "print(f\"Average F1 Score: {round(avg_f1, 3)}\")\n",
    "print(f\"Average Precision: {round(avg_precision, 3)}\")\n",
    "print(f\"Average Recall: {round(avg_recall, 3)}\")\n",
    "print(f\"Average ROC AUC: {round(avg_roc_auc, 3)}\")\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.plot(avg_train_accuracies, label='Average Train Accuracy', linewidth=2, marker='o', markersize=5)\n",
    "plt.plot(avg_val_accuracies, label='Average Validation Accuracy', linewidth=2, marker='s', markersize=5)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Average Model Accuracy Over Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "\n",
    "# Legend\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "# Set limits for x and y axes\n",
    "plt.xlim(0, epochs - 1)\n",
    "plt.ylim(0, 1)  # Assuming accuracy ranges between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, outside the MLflow run, average the ROC metrics and plot\n",
    "avg_fpr = np.linspace(0, 1, 100)\n",
    "avg_tpr = np.mean([np.interp(avg_fpr, f, t) for f, t in zip(fold_fpr, fold_tpr)], axis=0)\n",
    "avg_roc_auc = auc(avg_fpr, avg_tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(avg_fpr, avg_tpr, color='darkorange', lw=2,\n",
    "         label='Mean ROC curve (area = %0.2f)' % avg_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first collect the package versions\n",
    "import streamlit\n",
    "import xgboost\n",
    "import tensorflow\n",
    "import keras\n",
    "import sklearn\n",
    "import seaborn\n",
    "import matplotlib\n",
    "import os\n",
    "# Print package versions\n",
    "package_versions = {\n",
    "    \"streamlit\": streamlit.__version__,\n",
    "    \"joblib\": joblib.__version__,\n",
    "    \"xgboost\": xgboost.__version__,\n",
    "    \"tensorflow\": tensorflow.__version__,\n",
    "    \"keras\": keras.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"matplotlib\": matplotlib.__version__,\n",
    "    \"seaborn\": seaborn.__version__,\n",
    "}\n",
    "\n",
    "# Print versions\n",
    "for package, version in package_versions.items():\n",
    "    print(f\"{package}: {version}\")\n",
    "\n",
    "# Now let's create a requirements file\n",
    "requirements = \"\\n\".join([f\"{package}=={version}\" for package, version in package_versions.items() if version != \"Built-in Module\"])\n",
    "\n",
    "# Save to a requirements.txt file\n",
    "requirements_file_path = 'data/requirements.txt'\n",
    "with open(requirements_file_path, 'w') as file:\n",
    "    file.write(requirements)\n",
    "\n",
    "requirements_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender_pred_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
