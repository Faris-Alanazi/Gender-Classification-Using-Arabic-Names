{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python=3.8\n",
    "# conda env name: gender_pred_env\n",
    "# Packages\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import mlflow\n",
    "import os\n",
    "import optuna\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_pickle('data/dataset_after_preporcessing.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_name_length = max(df['name'].apply(len))\n",
    "unique_chars = set(''.join(df['name']))  \n",
    "vocab_size = len(unique_chars) + 1  \n",
    "\n",
    "# Tokenizing and padding the 'name' column for LSTM input\n",
    "tokenizer = Tokenizer(num_words=vocab_size, char_level=True)  # char_level=True for character tokenization\n",
    "tokenizer.fit_on_texts(df['name'])\n",
    "sequences = tokenizer.texts_to_sequences(df['name'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter = df['name'].apply(lambda x: x[0])\n",
    "last_letter = df['name'].apply(lambda x: x[-1])\n",
    "\n",
    "first_letter_encoded = np.array([ord(char) for char in first_letter])\n",
    "last_letter_encoded = np.array([ord(char) for char in last_letter])\n",
    "\n",
    "max_unicode_value_first = np.max(first_letter_encoded)\n",
    "max_unicode_value_last = np.max(last_letter_encoded)\n",
    "max_unicode_value = max(max_unicode_value_first, max_unicode_value_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler models\n",
    "scaler_first = joblib.load('saved_models/scaler_models/scaler_first_letter.pkl')\n",
    "scaler_last = joblib.load('saved_models/scaler_models/scaler_last_letter.pkl')\n",
    "\n",
    "# Transform the new data using the loaded scalers\n",
    "first_letter_encoded_scaled = scaler_first.transform(first_letter_encoded.reshape(-1, 1))\n",
    "last_letter_encoded_scaled = scaler_last.transform(last_letter_encoded.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sex'].values\n",
    "name_length = df['name_length'].values\n",
    "X = list(zip(padded_sequences, first_letter_encoded_scaled, last_letter_encoded_scaled, name_length))\n",
    "\n",
    "# Define the size for the test and validation sets as percentages\n",
    "test_size_percentage = 0.1\n",
    "validation_size_percentage = 0.1\n",
    "\n",
    "# Calculate the actual sizes for the test and validation sets\n",
    "total_size = test_size_percentage + validation_size_percentage\n",
    "test_size_actual = test_size_percentage / total_size\n",
    "validation_size_actual = validation_size_percentage / total_size\n",
    "\n",
    "train_size_percentage = 1 - total_size\n",
    "\n",
    "# First split: Separate out the training data and the remaining data\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=total_size, random_state=11)\n",
    "\n",
    "# Second split: Separate the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=test_size_actual, random_state=11)\n",
    "\n",
    "# Print the number of samples in the training, validation, and test sets\n",
    "print(f\"Training set size: {len(X_train)}, Labels: {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}, Labels: {len(y_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}, Labels: {len(y_test)}\")\n",
    "\n",
    "# Unpack the training data into separate arrays for each input\n",
    "name_train, first_letter_train, last_letter_train, length_train = zip(*X_train)\n",
    "name_val, first_letter_val, last_letter_val, length_val = zip(*X_val)\n",
    "name_test, first_letter_test, last_letter_test, length_test = zip(*X_test)\n",
    "\n",
    "# Convert tuples to numpy arrays\n",
    "name_train = np.array(name_train)\n",
    "first_letter_train = np.array(first_letter_train)\n",
    "last_letter_train = np.array(last_letter_train)\n",
    "length_train = np.array(length_train)\n",
    "\n",
    "name_val = np.array(name_val)\n",
    "first_letter_val = np.array(first_letter_val)\n",
    "last_letter_val = np.array(last_letter_val)\n",
    "length_val = np.array(length_val)\n",
    "\n",
    "name_test = np.array(name_test)\n",
    "first_letter_test = np.array(first_letter_test)\n",
    "last_letter_test = np.array(last_letter_test)\n",
    "length_test = np.array(length_test)\n",
    "\n",
    "# Reshape the length arrays to have two dimensions\n",
    "length_train = length_train.reshape(-1, 1)\n",
    "length_val = length_val.reshape(-1, 1)\n",
    "length_test = length_test.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the features\n",
    "X_train = np.concatenate([name_train, first_letter_train, last_letter_train, length_train], axis=1)\n",
    "X_val = np.concatenate([name_val, first_letter_val, last_letter_val, length_val], axis=1)\n",
    "X_test = np.concatenate([name_test, first_letter_test, last_letter_test, length_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 100),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 14),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 8),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "    }\n",
    "\n",
    "    # Initialize and train the ExtraTreesClassifier\n",
    "    model = ExtraTreesClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_preds)\n",
    "    \n",
    "    # We return the validation accuracy for the Optuna study to optimize\n",
    "    return val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')  # we use 'maximize' since we're looking to improve f1 score\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "best_extratrees_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Gender Prediction Models Tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Initialize ExtraTreesClassifier with best hyperparameters\n",
    "    model = ExtraTreesClassifier(**best_extratrees_params)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_params(best_extratrees_params)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "    \n",
    "    # Confusion matrix logging\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    cm_file = \"confusion_matrix.txt\"\n",
    "    np.savetxt(cm_file, cm, fmt='%d')\n",
    "    mlflow.log_artifact(cm_file)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Log additional information\n",
    "    mlflow.set_tags({\n",
    "        \"Description\": \"Optimized ExtraTrees binary classifier\",\n",
    "        \"Features\": ', '.join(df.columns.tolist()),\n",
    "        'Number of Features':len(df.columns.tolist()),\n",
    "        \"Encoding\" : \"Char Level For names | Label Encoding for other features\",\n",
    "        \"Model Type\": \"ExtraTrees\"\n",
    "    })\n",
    "\n",
    "    # Cleanup: Delete the saved confusion matrix file\n",
    "    os.remove(cm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all metrics\n",
    "print('\\n-----------------------------------------------------------')\n",
    "print(f\"Train Accuracy: {round(train_accuracy, 3)}\")\n",
    "print(f\"Validation Accuracy: {round(val_accuracy, 3)}\")\n",
    "print(f\"Test Accuracy: {round(test_accuracy, 3)}\")\n",
    "print(\"\\n---Metrics---\\n\")\n",
    "print(f\"F1 Score: {round(test_f1, 3)}\")\n",
    "print(f\"Precision: {round(test_precision, 3)}\")\n",
    "print(f\"Recall: {round(test_recall, 3)}\")\n",
    "print(f\"ROC AUC: {round(test_roc_auc, 3)}\")\n",
    "print('-----------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_accuracy > 0.870 or test_f1 > 0.92:\n",
    "    model_filename = f\"saved_models/ExtraTrees_Models/ExtraTrees_Acc_{round(test_accuracy,3)}_F1_{round(test_f1,3)}_Roc_{round(test_roc_auc,3)}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(\"Model Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
