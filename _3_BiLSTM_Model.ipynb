{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python=3.8\n",
    "# conda env name : gender_pred_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import optuna\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Adjusting warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_pickle('data/dataset_after_preporcessing.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_name_length = max(df['name'].apply(len))\n",
    "unique_chars = set(''.join(df['name']))  \n",
    "vocab_size = len(unique_chars) + 1  \n",
    "\n",
    "with open('saved_models/tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['name'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter = df['name'].apply(lambda x: x[0])\n",
    "last_letter = df['name'].apply(lambda x: x[-1])\n",
    "\n",
    "first_letter_encoded = np.array([ord(char) for char in first_letter])\n",
    "last_letter_encoded = np.array([ord(char) for char in last_letter])\n",
    "\n",
    "max_unicode_value_first = np.max(first_letter_encoded)\n",
    "max_unicode_value_last = np.max(last_letter_encoded)\n",
    "max_unicode_value = max(max_unicode_value_first, max_unicode_value_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler models\n",
    "scaler_first = joblib.load('saved_models/scaler_models/scaler_first_letter.pkl')\n",
    "scaler_last = joblib.load('saved_models/scaler_models/scaler_last_letter.pkl')\n",
    "\n",
    "# Transform the new data using the loaded scalers\n",
    "first_letter_encoded_scaled = scaler_first.transform(first_letter_encoded.reshape(-1, 1))\n",
    "last_letter_encoded_scaled = scaler_last.transform(last_letter_encoded.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sex'].values\n",
    "name_length = df['name_length'].values\n",
    "X = list(zip(padded_sequences, first_letter_encoded_scaled, last_letter_encoded_scaled, name_length))\n",
    "\n",
    "# Define the size for the test and validation sets as percentages\n",
    "test_size_percentage = 0.1\n",
    "validation_size_percentage = 0.1\n",
    "\n",
    "# Calculate the actual sizes for the test and validation sets\n",
    "total_size = test_size_percentage + validation_size_percentage\n",
    "test_size_actual = test_size_percentage / total_size\n",
    "validation_size_actual = validation_size_percentage / total_size\n",
    "\n",
    "train_size_percentage = 1 - total_size\n",
    "\n",
    "# First split: Separate out the training data and the remaining data\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=total_size, random_state=11)\n",
    "\n",
    "# Second split: Separate the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=test_size_actual, random_state=11)\n",
    "\n",
    "# Print the number of samples in the training, validation, and test sets\n",
    "print(f\"Training set size: {len(X_train)}, Labels: {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}, Labels: {len(y_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}, Labels: {len(y_test)}\")\n",
    "\n",
    "# Unpack the training data into separate arrays for each input\n",
    "name_train, first_letter_train, last_letter_train, length_train = zip(*X_train)\n",
    "name_val, first_letter_val, last_letter_val, length_val = zip(*X_val)\n",
    "name_test, first_letter_test, last_letter_test, length_test = zip(*X_test)\n",
    "\n",
    "# Convert tuples to numpy arrays\n",
    "name_train = np.array(name_train)\n",
    "first_letter_train = np.array(first_letter_train)\n",
    "last_letter_train = np.array(last_letter_train)\n",
    "length_train = np.array(length_train)\n",
    "\n",
    "name_val = np.array(name_val)\n",
    "first_letter_val = np.array(first_letter_val)\n",
    "last_letter_val = np.array(last_letter_val)\n",
    "length_val = np.array(length_val)\n",
    "\n",
    "name_test = np.array(name_test)\n",
    "first_letter_test = np.array(first_letter_test)\n",
    "last_letter_test = np.array(last_letter_test)\n",
    "length_test = np.array(length_test)\n",
    "\n",
    "# Reshape the length arrays to have two dimensions\n",
    "length_train = length_train.reshape(-1, 1)\n",
    "length_val = length_val.reshape(-1, 1)\n",
    "length_test = length_test.reshape(-1, 1)\n",
    "\n",
    "# Concatenate the features\n",
    "X_train = np.concatenate([name_train, first_letter_train, last_letter_train, length_train], axis=1)\n",
    "X_val = np.concatenate([name_val, first_letter_val, last_letter_val, length_val], axis=1)\n",
    "X_test = np.concatenate([name_test, first_letter_test, last_letter_test, length_test], axis=1)\n",
    "total_features_shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lstm_objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna for BiLSTM\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [32, 64, 128, 256,512])\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [32, 64, 128, 256])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
    "    l2_lambda = trial.suggest_loguniform('l2_reg', 1e-6, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    # epochs = trial.suggest_int('epochs', 5, 50)\n",
    "    patience_for_early_stopping = trial.suggest_int('patience_for_early_stopping', 5, 15)\n",
    "\n",
    "    # Define the model architecture using the hyperparameters\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=total_features_shape))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(lstm_units,return_sequences=True,kernel_regularizer=l2(l2_lambda))))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(lstm_units//2,kernel_regularizer=l2(l2_lambda))))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Use early stopping as a callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience_for_early_stopping)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # The objective we want to minimize (validation loss in this case)\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    \n",
    "    # Optionally, you can return additional information to be used later\n",
    "    trial.set_user_attr('stopped_epoch', len(history.history['loss']))\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lstm_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "best_lstm_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_summary(model):\n",
    "    \"\"\"\n",
    "    Generates the summary of the given Keras model and returns it as a string.\n",
    "\n",
    "    Args:\n",
    "    model (keras.Model): The Keras model to summarize.\n",
    "\n",
    "    Returns:\n",
    "    str: The summary of the model.\n",
    "    \"\"\"\n",
    "    model_summary_list = []\n",
    "    model.summary(print_fn=lambda x: model_summary_list.append(x))\n",
    "    return '\\n'.join(model_summary_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Gender Prediction Models Tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow tracking\n",
    "with mlflow.start_run():\n",
    "    # Log model parameters\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"max_name_length\": max_name_length,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"train_size_percentage\": train_size_percentage,\n",
    "        \"test_size_percentage\": test_size_percentage,\n",
    "        \"validation_size_percentage\": validation_size_percentage\n",
    "    })\n",
    "    \n",
    "    mlflow.log_params(best_lstm_params)\n",
    "\n",
    "    # Define the model architecture using the hyperparameters\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=best_lstm_params['embedding_dim'], input_length=total_features_shape))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(best_lstm_params['lstm_units'],return_sequences=True,kernel_regularizer=l2(best_lstm_params['l2_reg']))))\n",
    "\n",
    "    model.add(Dropout(best_lstm_params['dropout_rate']))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(best_lstm_params['lstm_units']//2,kernel_regularizer=l2(best_lstm_params['l2_reg']))))\n",
    "\n",
    "    model.add(Dropout(best_lstm_params['dropout_rate']))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=best_lstm_params['patience_for_early_stopping'])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=best_lstm_params['batch_size'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Log training history\n",
    "    for epoch in range(len(history.history['accuracy'])):\n",
    "        mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "    # Predictions and additional metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.where(y_pred > 0.5, 1, 0).reshape(-1)\n",
    "    f1 = f1_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes)\n",
    "    recall = recall_score(y_test, y_pred_classes)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    # Calculate AUC (Area Under the ROC Curve)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Confusion matrix calculation\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    cm_dict = {f\"{i}-{j}\": cm[i, j] for i in range(cm.shape[0]) for j in range(cm.shape[1])}\n",
    "\n",
    "    # Log additional metrics\n",
    "    mlflow.log_metrics({\n",
    "    \"test_f1\": f1,\n",
    "    \"test_precision\": precision,\n",
    "    \"test_recall\": recall,\n",
    "    \"test_roc_auc\": roc_auc,\n",
    "    \"test_loss\": test_loss, \n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"train_accuracy\":train_accuracy,\n",
    "    \"train_loss\":train_loss,\n",
    "    \"val_loss\":val_loss,\n",
    "    \"val_accuracy\":val_accuracy\n",
    "    })\n",
    "\n",
    "    # Infer the signature using the combined data and predictions\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "    mlflow.keras.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "    # Set additional tags\n",
    "    mlflow.set_tags({\n",
    "        \"Description\": \"Character-Level BiLSTM\",\n",
    "        \"Encoding\": \"Character-Level for name | other featuers Coverting them to thier ASCI value then norm\",\n",
    "        \"Features\": ', '.join(df.columns.tolist()),\n",
    "        'Number of Features':len(df.columns.tolist()),\n",
    "        \"Model Type\": \"BiLSTM\",\n",
    "        \"model_architecture\": get_model_summary(model),\n",
    "        'confusion_matrix':list(cm_dict.values())\n",
    "    })\n",
    "# End MLflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('-----------------------------------------------------------')\n",
    "print(f\"Train Accuracy: {round(train_accuracy, 3)}\")\n",
    "print(f\"Val Accuracy: {round(val_accuracy, 3)}\")\n",
    "print(f\"Test Accuracy: {round(test_accuracy, 3)}\")\n",
    "print(\"\\n---Metrics---\\n\")\n",
    "print(f\"F1 Score: {round(f1, 3)}\")\n",
    "print(f\"Precision: {round(precision, 3)}\")\n",
    "print(f\"Recall: {round(recall, 3)}\")\n",
    "print(f\"ROC AUC: {round(roc_auc, 3)}\")\n",
    "print(\"\\n---Loss---\\n\")\n",
    "print(f\"Train Loss: {round(train_loss, 3)}\")\n",
    "print(f\"Val Loss: {round(val_loss, 3)}\")\n",
    "print(f\"Test Loss: {round(test_loss, 3)}\")\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style to make plots nicer\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o', markersize=5)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s', markersize=5)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Model Accuracy Over Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "\n",
    "# Legend\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "# Set limits and ticks for readability\n",
    "plt.xlim(0, len(history.history['accuracy']) - 1)\n",
    "plt.ylim(0.7, max(max(history.history['accuracy']), max(history.history['val_accuracy'])) + 0.05)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label');\n",
    "list(cm_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_accuracy > 0.870 or f1 > 0.92:\n",
    "    xgb_model.save_model(f\"saved_models/BiLSTM Models/BiLSTM_Acc_{round(test_accuracy,3)}_F1_{round(f1,3)}_Roc_{round(roc_auc,3)}.h5\")\n",
    "    print(\"model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
